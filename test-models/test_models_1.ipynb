{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab1468f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COPY TO EXCEL ===\n",
      "\n",
      "2026-02-10 20:54:40\tRami\tNone\t```python\n",
      "import cmath\n",
      "\n",
      "def solve_quadratic_equation(a, b, c):\n",
      "  \"\"\"\n",
      "  Solves a quadratic equation of the form ax^2 + bx + c = 0.\n",
      "\n",
      "  Args:\n",
      "    a: The \t19.06 sec\t14029 MB\t-0.61 GB\t2.9 %\tWindows PC (RTX 5070 Ti, Ryzen 9 9950X, 16GB RAM)\tWrite a Python function to solve a quadratic equation.\tgemma3:12b\n",
      "2026-02-10 20:54:48\tRami\tNone\t**Artificial Intelligence (AI)**: Think of AI as a super smart robot that can think, learn, and act like a human being. It's designed to perform tasks\t7.67 sec\t9778 MB\t-1.08 GB\t6.1 %\tWindows PC (RTX 5070 Ti, Ryzen 9 9950X, 16GB RAM)\tExplain the difference between AI and Machine Learning in simple terms.\tllama3.1:8b\n",
      "2026-02-10 20:54:49\tRami\tNone\tEmbedding size: 768\t0.83 sec\t10531 MB\t0.5 GB\t4.1 %\tWindows PC (RTX 5070 Ti, Ryzen 9 9950X, 16GB RAM)\tHello world\tnomic-embed-text\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import time\n",
    "import psutil\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "TESTER_NAME = \"Rami\"\n",
    "DEVICE = \"Windows PC (RTX 5070 Ti, Ryzen 9 9950X, 32GB RAM)\"\n",
    "\n",
    "tests = [\n",
    "    {\n",
    "        \"model\": \"gemma3:12b\",\n",
    "        \"prompt\": \"Write a Python function to solve a quadratic equation.\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"llama3.1:8b\",\n",
    "        \"prompt\": \"Explain the difference between AI and Machine Learning in simple terms.\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"nomic-embed-text\",\n",
    "        \"prompt\": \"Hello world\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def get_vram_usage():\n",
    "    \"\"\"Get current GPU memory usage via nvidia-smi (not peak).\"\"\"\n",
    "    try:\n",
    "        result = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.used\", \"--format=csv,noheader,nounits\"],\n",
    "            shell=False\n",
    "        )\n",
    "        return result.decode().strip() + \" MB\"\n",
    "    except Exception:\n",
    "        return \"N/A\"\n",
    "\n",
    "def test_model(test):\n",
    "    model = test[\"model\"]\n",
    "    prompt = test[\"prompt\"]\n",
    "\n",
    "    start_time = time.time()\n",
    "    ram_before = psutil.virtual_memory().used / (1024 ** 3)\n",
    "\n",
    "    errors = \"\"\n",
    "    output_sample = \"\"\n",
    "\n",
    "    try:\n",
    "        if \"embed\" in model:\n",
    "            response = ollama.embeddings(model=model, prompt=prompt)\n",
    "            output_sample = f\"Embedding size: {len(response['embedding'])}\"\n",
    "        else:\n",
    "            response = ollama.chat(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            output_sample = response[\"message\"][\"content\"][:150]\n",
    "\n",
    "    except Exception as e:\n",
    "        errors = str(e)\n",
    "\n",
    "    response_time = round(time.time() - start_time, 2)\n",
    "    ram_after = psutil.virtual_memory().used / (1024 ** 3)\n",
    "\n",
    "    # Better CPU measurement: sample over a short interval\n",
    "    cpu_usage = psutil.cpu_percent(interval=0.2)\n",
    "\n",
    "    return {\n",
    "        \"Date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"Tester\": TESTER_NAME,\n",
    "        \"Errors/Warnings\": errors or \"None\",\n",
    "        \"Output Sample\": output_sample,\n",
    "        \"Response Time\": f\"{response_time} sec\",\n",
    "        \"VRAM Usage\": get_vram_usage(),\n",
    "        \"RAM Usage\": f\"{round(ram_after - ram_before, 2)} GB\",\n",
    "        \"CPU Usage\": f\"{cpu_usage} %\",\n",
    "        \"Device Used\": DEVICE,\n",
    "        \"Prompt Used\": prompt,\n",
    "        \"Model Name\": model,\n",
    "    }\n",
    "\n",
    "results = [test_model(t) for t in tests]\n",
    "\n",
    "print(\"\\n=== COPY TO EXCEL ===\\n\")\n",
    "for r in results:\n",
    "    print(\"\\t\".join(str(v) for v in r.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407f6643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Appended 4 rows to: C:\\Users\\omarch\\Desktop\\Rami\\projects\\offline_Ai\\test_models\\Models Report.xlsx\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import time\n",
    "import psutil\n",
    "import subprocess\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import openpyxl\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "REPORT_PATH = r\"Models Report.xlsx\"   # <-- put your Excel file in the same folder OR set full path here\n",
    "TESTER_NAME = \"Rami\"\n",
    "DEVICE = \"Windows PC (RTX 5070 Ti, Ryzen 9 9950X, 32GB RAM)\"\n",
    "\n",
    "# Models to test\n",
    "CHAT_MODEL = \"gemma3:12b\"\n",
    "EMBED_MODEL = \"nomic-embed-text\"  # change to \"bge-m3\" if you use it\n",
    "EMBED_MODEL = \"bge-m3\"  # change to \"bge-m3\" if you use it\n",
    "\n",
    "# Strong prompts to stress a chat model (reasoning + structure + instruction-following)\n",
    "CHAT_TESTS = [\n",
    "    {\n",
    "        \"name\": \"Policy Reasoning + SOP\",\n",
    "        \"prompt\": (\n",
    "            \"You are an enterprise AI assistant inside an air-gapped company.\\n\"\n",
    "            \"Task: Create an internal SOP for handling a suspected data leak incident.\\n\"\n",
    "            \"Requirements:\\n\"\n",
    "            \"1) Provide exactly 7 numbered steps.\\n\"\n",
    "            \"2) Add a short 'Decision Table' with 3 rows: Severity (Low/Med/High) -> Action.\\n\"\n",
    "            \"3) Keep tone professional, no dramatic language.\\n\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Deep reasoning (trade-offs)\",\n",
    "        \"prompt\": (\n",
    "            \"We must choose between two options for internal knowledge:\\n\"\n",
    "            \"A) RAG with vector database (ChromaDB) and embeddings\\n\"\n",
    "            \"B) Fine-tuning using LoRA on internal chat logs\\n\\n\"\n",
    "            \"Compare A vs B across: Security, Maintenance, Accuracy, Cost, Upgrade flexibility.\\n\"\n",
    "            \"Then recommend one for the first 3 months, and justify in 5 bullets.\\n\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Format obedience (JSON only)\",\n",
    "        \"prompt\": (\n",
    "            \"Return ONLY valid JSON (no markdown, no extra text).\\n\"\n",
    "            \"Create a JSON object with keys:\\n\"\n",
    "            \"summary (string), risks (array of 4 strings), mitigations (array of 4 strings).\\n\"\n",
    "            \"Topic: Deploying local LLMs on an air-gapped network for 10+ users.\\n\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "EMBED_TEST = {\n",
    "    \"name\": \"Embedding sanity\",\n",
    "    \"prompt\": \"Company annual leave policy and approval workflow.\"\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# METRICS HELPERS\n",
    "# =========================\n",
    "def _query_vram_mb():\n",
    "    \"\"\"Return current GPU VRAM used in MB (string->int). Works if nvidia-smi is available.\"\"\"\n",
    "    try:\n",
    "        out = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.used\", \"--format=csv,noheader,nounits\"],\n",
    "            stderr=subprocess.DEVNULL\n",
    "        )\n",
    "        # If multiple GPUs, this returns multiple lines; take the max.\n",
    "        vals = [int(x.strip()) for x in out.decode().strip().splitlines() if x.strip().isdigit()]\n",
    "        return max(vals) if vals else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "class PeakSampler:\n",
    "    \"\"\"Samples VRAM and CPU/RAM while a model call is running.\"\"\"\n",
    "    def __init__(self, interval=0.2):\n",
    "        self.interval = interval\n",
    "        self._stop = threading.Event()\n",
    "        self.peak_vram = None\n",
    "        self.peak_ram_gb = None\n",
    "        self.peak_cpu = None\n",
    "\n",
    "    def start(self):\n",
    "        self._stop.clear()\n",
    "        t = threading.Thread(target=self._run, daemon=True)\n",
    "        t.start()\n",
    "        self._thread = t\n",
    "\n",
    "    def stop(self):\n",
    "        self._stop.set()\n",
    "        self._thread.join(timeout=2)\n",
    "\n",
    "    def _run(self):\n",
    "        p = psutil.Process()\n",
    "        while not self._stop.is_set():\n",
    "            # VRAM\n",
    "            v = _query_vram_mb()\n",
    "            if v is not None:\n",
    "                self.peak_vram = v if self.peak_vram is None else max(self.peak_vram, v)\n",
    "\n",
    "            # RAM (system used)\n",
    "            ram_gb = psutil.virtual_memory().used / (1024 ** 3)\n",
    "            self.peak_ram_gb = ram_gb if self.peak_ram_gb is None else max(self.peak_ram_gb, ram_gb)\n",
    "\n",
    "            # CPU (process %)\n",
    "            cpu = p.cpu_percent(interval=None)\n",
    "            self.peak_cpu = cpu if self.peak_cpu is None else max(self.peak_cpu, cpu)\n",
    "\n",
    "            time.sleep(self.interval)\n",
    "\n",
    "def test_chat_model(model: str, prompt: str):\n",
    "    sampler = PeakSampler(interval=0.2)\n",
    "    errors = \"\"\n",
    "    output_sample = \"\"\n",
    "\n",
    "    start = time.time()\n",
    "    sampler.start()\n",
    "    try:\n",
    "        resp = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        output_sample = resp[\"message\"][\"content\"][:250]\n",
    "    except Exception as e:\n",
    "        errors = str(e)\n",
    "    finally:\n",
    "        sampler.stop()\n",
    "\n",
    "    elapsed = round(time.time() - start, 2)\n",
    "\n",
    "    return {\n",
    "        \"Errors/Warnings\": errors or \"None\",\n",
    "        \"Output Sample\": output_sample,\n",
    "        \"Response Time\": f\"{elapsed} sec\",\n",
    "        \"VRAM Usage\": f\"{sampler.peak_vram} MB\" if sampler.peak_vram is not None else \"N/A\",\n",
    "        \"RAM Usage\": f\"{round(sampler.peak_ram_gb, 2)} GB\" if sampler.peak_ram_gb is not None else \"N/A\",\n",
    "        \"CPU Usage\": f\"{round(sampler.peak_cpu, 1)} %\" if sampler.peak_cpu is not None else \"N/A\",\n",
    "    }\n",
    "\n",
    "def test_embed_model(model: str, prompt: str):\n",
    "    sampler = PeakSampler(interval=0.2)\n",
    "    errors = \"\"\n",
    "    output_sample = \"\"\n",
    "\n",
    "    start = time.time()\n",
    "    sampler.start()\n",
    "    try:\n",
    "        resp = ollama.embeddings(model=model, prompt=prompt)\n",
    "        output_sample = f\"Embedding size: {len(resp['embedding'])}\"\n",
    "    except Exception as e:\n",
    "        errors = str(e)\n",
    "    finally:\n",
    "        sampler.stop()\n",
    "\n",
    "    elapsed = round(time.time() - start, 2)\n",
    "\n",
    "    return {\n",
    "        \"Errors/Warnings\": errors or \"None\",\n",
    "        \"Output Sample\": output_sample,\n",
    "        \"Response Time\": f\"{elapsed} sec\",\n",
    "        \"VRAM Usage\": f\"{sampler.peak_vram} MB\" if sampler.peak_vram is not None else \"N/A\",\n",
    "        \"RAM Usage\": f\"{round(sampler.peak_ram_gb, 2)} GB\" if sampler.peak_ram_gb is not None else \"N/A\",\n",
    "        \"CPU Usage\": f\"{round(sampler.peak_cpu, 1)} %\" if sampler.peak_cpu is not None else \"N/A\",\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# EXCEL WRITER\n",
    "# =========================\n",
    "HEADERS = [\n",
    "    \"Date\",\"Tester\",\"Errors/Warnings\",\"Output Sample\",\"Response Time\",\n",
    "    \"VRAM Usage\",\"RAM Usage\",\"CPU Usage\",\"Device Used\",\"Prompt Used\",\"Model Name\"\n",
    "]\n",
    "\n",
    "def append_to_excel(report_path: str, rows: list[dict]):\n",
    "    report_file = Path(report_path)\n",
    "    if not report_file.exists():\n",
    "        raise FileNotFoundError(f\"Excel file not found: {report_file.resolve()}\")\n",
    "\n",
    "    wb = openpyxl.load_workbook(report_file)\n",
    "    ws = wb.active\n",
    "\n",
    "    # Validate header row\n",
    "    existing_headers = [ws.cell(1, c).value for c in range(1, ws.max_column + 1)]\n",
    "    if existing_headers[:len(HEADERS)] != HEADERS:\n",
    "        # If sheet is empty or headers mismatch, write headers\n",
    "        for i, h in enumerate(HEADERS, start=1):\n",
    "            ws.cell(1, i).value = h\n",
    "\n",
    "    start_row = ws.max_row + 1\n",
    "    for i, r in enumerate(rows):\n",
    "        row_idx = start_row + i\n",
    "        for col_idx, h in enumerate(HEADERS, start=1):\n",
    "            ws.cell(row_idx, col_idx).value = r.get(h, \"\")\n",
    "\n",
    "    wb.save(report_file)\n",
    "\n",
    "# =========================\n",
    "# RUN TESTS\n",
    "# =========================\n",
    "def main():\n",
    "    rows = []\n",
    "\n",
    "    # Chat tests (gemma2:12b)\n",
    "    for t in CHAT_TESTS:\n",
    "        metrics = test_chat_model(CHAT_MODEL, t[\"prompt\"])\n",
    "        rows.append({\n",
    "            \"Date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"Tester\": TESTER_NAME,\n",
    "            \"Errors/Warnings\": metrics[\"Errors/Warnings\"],\n",
    "            \"Output Sample\": f\"[{t['name']}] {metrics['Output Sample']}\",\n",
    "            \"Response Time\": metrics[\"Response Time\"],\n",
    "            \"VRAM Usage\": metrics[\"VRAM Usage\"],\n",
    "            \"RAM Usage\": metrics[\"RAM Usage\"],\n",
    "            \"CPU Usage\": metrics[\"CPU Usage\"],\n",
    "            \"Device Used\": DEVICE,\n",
    "            \"Prompt Used\": t[\"prompt\"],\n",
    "            \"Model Name\": CHAT_MODEL,\n",
    "        })\n",
    "\n",
    "    # Embedding test\n",
    "    metrics = test_embed_model(EMBED_MODEL, EMBED_TEST[\"prompt\"])\n",
    "    rows.append({\n",
    "        \"Date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"Tester\": TESTER_NAME,\n",
    "        \"Errors/Warnings\": metrics[\"Errors/Warnings\"],\n",
    "        \"Output Sample\": f\"[{EMBED_TEST['name']}] {metrics['Output Sample']}\",\n",
    "        \"Response Time\": metrics[\"Response Time\"],\n",
    "        \"VRAM Usage\": metrics[\"VRAM Usage\"],\n",
    "        \"RAM Usage\": metrics[\"RAM Usage\"],\n",
    "        \"CPU Usage\": metrics[\"CPU Usage\"],\n",
    "        \"Device Used\": DEVICE,\n",
    "        \"Prompt Used\": EMBED_TEST[\"prompt\"],\n",
    "        \"Model Name\": EMBED_MODEL,\n",
    "    })\n",
    "\n",
    "    append_to_excel(REPORT_PATH, rows)\n",
    "    print(f\"Done. Appended {len(rows)} rows to: {Path(REPORT_PATH).resolve()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22a7e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL: gemma3:12b\n",
      "PROMPT:\n",
      " أنت مساعد ذكاء اصطناعي يعمل داخل شركة تعتمد نظام ذكاء اصطناعي محلي.\n",
      "اكتب إجراء تشغيلي قياسي (SOP) للتعامل مع حادث تسريب بيانات محتمل.\n",
      "المتطلبات:\n",
      "1) سبع خطوات مرقمة فقط.\n",
      "2) جدول قرار مبسط يحتوي على (منخفض، متوسط، عالي).\n",
      "3) لغة رسمية واحترافية بدون مبالغة.\n",
      "\n",
      "\n",
      "RESPONSE:\n",
      " ## إجراء تشغيلي قياسي (SOP) للتعامل مع حادث تسريب بيانات محتمل\n",
      "\n",
      "**الهدف:** توثيق الإجراءات اللازمة لتحديد وتقييم والاستجابة لحوادث تسريب البيانات المحتملة داخل الشركة، مع ضمان تقليل الضرر المحتمل والالتزام بالمتطلبات القانونية والتنظيمية.\n",
      "\n",
      "**النطاق:** يغطي هذا الإجراء جميع موظفي الشركة وأي طرف ثالث له حق الوصول إلى بيانات الشركة.\n",
      "\n",
      "**1. اكتشاف وإبلاغ:** عند الاشتباه في حدوث تسريب بيانات، يجب على الموظف المسؤول الإبلاغ الفوري إلى قسم تكنولوجيا المعلومات والأمن السيبراني عبر القناة المحددة (مثل البريد الإلكتروني المخصص أو رقم الهاتف). يجب تضمين أكبر قدر ممكن من التفاصيل حول الحادث المشتبه به (البيانات المتأثرة المحتملة، كيفية الاكتشاف، الخ).\n",
      "\n",
      "**2. احتواء الحادث:** يقوم فريق تكنولوجيا المعلومات والأمن السيبراني فورًا بتقييم المعلومات الأولية والبدء في خطوات الاحتواء لمنع انتشار التسريب المحتمل. قد تتضمن هذه الخطوات عزل الأنظمة المتأثرة، وتعطيل الوصول إلى البيانات، وتغيير كلمات المرور.\n",
      "\n",
      "**3. تقييم الأثر:** يتم إجراء تقييم مفصل لتحديد البيانات المتأثرة وعدد السجلات المتضررة، بالإضافة إلى تحديد أسباب التسريب المحتملة ونقاط الضعف التي تم استغلالها. يتم تصنيف شدة الحادث بناءً على الجدول التالي:\n",
      "\n",
      "**جدول تقييم شدة الحادث:**\n",
      "\n",
      "| مستوى الشدة | وصف الأثر المحتمل |  الاستجابة المطلوبة |\n",
      "|---|---|---|\n",
      "| منخفض | بيانات غير حساسة، عدد محدود من السجلات المتأثرة، تأثير ضئيل على العمليات. | توثيق الحادث، مراجعة الإجراءات الوقائية، إبلاغ الإدارة المعنية. |\n",
      "| متوسط | بيانات حساسة محدودة، عدد متوسط من السجلات المتأثرة، تأثير محتمل على العمليات و/أو السمعة. | توثيق الحادث، تطبيق إجراءات تصحيحية فورية، إبلاغ الإدارة التنفيذية، إعداد خطة اتصال أولية. |\n",
      "| عالي | بيانات حساسة عالية، عدد كبير من السجلات المتأثرة، تأثير كبير على العمليات و/أو السمعة والالتزام القانوني. | تفعيل خطة الاستجابة لحوادث البيانات، تطبيق إجراءات تصحيحية عاجلة، إبلاغ الإدارة التنفيذية والجهات التنظيمية (إذا لزم الأمر)، تشكيل فريق استجابة أزمة. |\n",
      "\n",
      "**4. التحقيق:** يقوم فريق متخصص في الأمن السيبراني بإجراء تحقيق شامل لتحديد السبب الجذري للتسريب، وتحديد الفاعلين، وجمع الأدلة. يجب توثيق جميع خطوات التحقيق.\n",
      "\n",
      "**5. التعافي:** بعد احتواء الحادث والتحقيق فيه، يتم تنفيذ إجراءات التعافي لإعادة الأنظمة المتأثرة إلى حالتها الطبيعية. يتضمن ذلك استعادة البيانات (إذا أمكن)، وتصحيح نقاط الضعف، وتعزيز الإجراءات الأمنية.\n",
      "\n",
      "**6. الإبلاغ (Reporting):** يتم إعداد تقرير مفصل عن الحادث، يشمل تفاصيل حول الأسباب، والإجراءات المتخذة، والدروس المستفادة، والتوصيات للوقاية من الحوادث المستقبلية. يجب توزيع التقرير على الإدارة المعنية وأصحاب المصلحة الآخرين.\n",
      "\n",
      "**7. المراجعة والتحسين:** يتم مراجعة هذا الإجراء بشكل دوري (على الأقل سنويًا) أو عند حدوث تغييرات جوهرية في البنية التحتية لتكنولوجيا المعلومات أو المتطلبات القانونية والتنظيمية، بهدف التحسين المستمر لعمليات الاستجابة لحوادث تسريب البيانات.\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "MODEL: gemma3:12b\n",
      "PROMPT:\n",
      " لدينا خياران لإدارة المعرفة الداخلية:\n",
      "أ) استخدام RAG مع قاعدة بيانات متجهات.\n",
      "ب) استخدام Fine-Tuning عبر LoRA.\n",
      "\n",
      "قارن بين الخيارين من حيث:\n",
      "الأمان، الصيانة، الدقة، التكلفة، وسهولة الترقية.\n",
      "ثم قدم توصية واضحة مع تبرير منطقي من 5 نقاط.\n",
      "\n",
      "RESPONSE:\n",
      " بالتأكيد، لنقارن بين خياري إدارة المعرفة الداخلية (RAG مع قاعدة بيانات متجهات و Fine-Tuning عبر LoRA) بناءً على المعايير التي ذكرتها، ثم نقدم توصية مع تبرير.\n",
      "\n",
      "**مقارنة بين RAG مع قاعدة بيانات متجهات و Fine-Tuning عبر LoRA**\n",
      "\n",
      "| المعيار | RAG مع قاعدة بيانات متجهات | Fine-Tuning عبر LoRA |\n",
      "|---|---|---|\n",
      "| **الأمان** |  أكثر أمانًا. البيانات الأصلية لا تتغير، الاستعلامات مقيدة بمحتوى قاعدة البيانات.  | أقل أمانًا.  تعديل النموذج الأساسي يمكن أن يؤدي إلى كشف معلومات حساسة أو سلوك غير مرغوب فيه. |\n",
      "| **الصيانة** |  صيانة أسهل. تحديثات قاعدة البيانات بسيطة نسبيًا. |  صيانة أكثر تعقيدًا.  تتطلب إعادة تدريب النموذج عند إضافة بيانات جديدة أو تغيير السياسات. |\n",
      "| **الدقة** | تعتمد على جودة قاعدة البيانات وجودة الاستعلامات. يمكن أن تكون محدودة في فهم السياق المعقد. |  يمكن أن تكون أكثر دقة في المهام المتخصصة إذا تم التدريب بشكل جيد. |\n",
      "| **التكلفة** |  أقل تكلفة في البداية. تكاليف التشغيل تعتمد على حجم قاعدة البيانات واستعلامات المستخدمين. |  أعلى تكلفة في البداية (تدريب النموذج). تكاليف التشغيل تعتمد على حجم النموذج والمهام. |\n",
      "| **سهولة الترقية** |  أسهل. يمكن استبدال قاعدة البيانات أو نموذج تضمين المتجهات دون التأثير على النموذج اللغوي. |  أكثر صعوبة. تتطلب ترقية النموذج اللغوي الأساسي أو إعادة تدريب LoRA. |\n",
      "\n",
      "**شرح تفصيلي لكل معيار:**\n",
      "\n",
      "*   **الأمان:** في RAG، لا يتم تعديل النموذج اللغوي نفسه. يتم استرداد المعلومات من قاعدة بيانات منفصلة، مما يقلل من خطر تسرب البيانات أو السلوك غير المرغوب فيه. أما في Fine-Tuning، فإنك تعدل النموذج الأساسي، مما يزيد من خطر دمج بيانات حساسة في النموذج.\n",
      "*   **الصيانة:** RAG أسهل في الصيانة لأنك فقط بحاجة إلى تحديث قاعدة البيانات. أما Fine-Tuning يتطلب إعادة تدريب النموذج بشكل دوري، وهو ما يستغرق وقتًا وجهدًا.\n",
      "*   **الدقة:** RAG يعتمد على قدرة النموذج على فهم الاستعلامات واسترجاع المعلومات ذات الصلة من قاعدة البيانات. يمكن أن يكون محدودًا في فهم السياق المعقد. Fine-Tuning يتيح لك تدريب النموذج على مهام محددة، مما قد يؤدي إلى دقة أعلى.\n",
      "*   **التكلفة:** RAG أقل تكلفة في البداية، حيث لا يتطلب تدريب نموذج جديد. Fine-Tuning يتطلب موارد حاسوبية كبيرة لتدريب النموذج.\n",
      "*   **سهولة الترقية:** RAG أسهل في الترقية لأنك يمكنك تحديث قاعدة البيانات أو نموذج تضمين المتجهات دون التأثير على النموذج اللغوي. Fine-Tuning أكثر صعوبة لأنه يتطلب ترقية النموذج اللغوي الأساسي أو إعادة تدريب LoRA.\n",
      "\n",
      "**التوصية: RAG مع قاعدة بيانات متجهات**\n",
      "\n",
      "أوصي باستخدام **RAG مع قاعدة بيانات متجهات** لإدارة المعرفة الداخلية، وذلك للأسباب التالية:\n",
      "\n",
      "1.  **الأمان:** الأمان هو الأولوية القصوى عند التعامل مع المعرفة الداخلية الحساسة. RAG يوفر مستوى أعلى من الأمان مقارنة بـ Fine-Tuning.\n",
      "2.  **سهولة الصيانة:** إدارة المعرفة تتطلب تحديثات مستمرة. RAG يسهل هذه العملية مقارنة بـ Fine-Tuning.\n",
      "3.  **فعالية التكلفة:**  بالنسبة لمعظم الحالات، فإن RAG يوفر حلاً أكثر فعالية من حيث التكلفة، خاصة في المراحل الأولية.\n",
      "4.  **المرونة:** RAG أكثر مرونة وقابلية للتكيف مع التغييرات في المعرفة الداخلية.\n",
      "5.  **التقليل من المخاطر:** تجنب تعديل النموذج اللغوي الأساسي يقلل من خطر إدخال سلوك غير مرغوب فيه أو كشف معلومات حساسة.\n",
      "\n",
      "**متى يمكن النظر في Fine-Tuning؟**\n",
      "\n",
      "إذا كانت لديك ميزانية كبيرة، وكنت بحاجة إلى دقة استثنائية في مهام محددة جدًا، وكنت على استعداد لتحمل مخاطر أمنية أكبر وتكاليف صيانة أعلى، فقد يكون Fine-Tuning عبر LoRA خيارًا. ومع ذلك، في معظم الحالات، فإن RAG يوفر حلاً أكثر عملية وفعالية لإدارة المعرفة الداخلية.\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "MODEL: gemma3:12b\n",
      "PROMPT:\n",
      " أعد النتيجة بصيغة JSON فقط بدون أي نص إضافي.\n",
      "المفاتيح المطلوبة:\n",
      "summary (string), risks (array of 4 strings), mitigations (array of 4 strings).\n",
      "الموضوع: نشر نموذج ذكاء اصطناعي محلي داخل بيئة Air-Gapped.\n",
      "\n",
      "RESPONSE:\n",
      " ```json\n",
      "{\n",
      "  \"summary\": \"نشر نموذج ذكاء اصطناعي محلي داخل بيئة Air-Gapped يتطلب تخطيطًا دقيقًا لضمان الأمان والتشغيل السليم، مع مراعاة القيود المفروضة على الاتصالات الخارجية. يجب نقل النموذج والبيانات اللازمة بشكل آمن، وتحديثات الأمان، والتأكد من أن البنية التحتية المحلية قادرة على دعم متطلبات النموذج.\",\n",
      "  \"risks\": [\n",
      "    \"إدخال برامج ضارة أثناء نقل النموذج والبيانات.\",\n",
      "    \"عدم وجود تحديثات أمنية مستمرة للنموذج والبرامج التابعة.\",\n",
      "    \"نقص في المهارات أو الخبرة لإدارة وتشغيل النموذج في بيئة معزولة.\",\n",
      "    \"مشاكل في الأداء أو الاستقرار بسبب محدودية الموارد المحلية.\"\n",
      "  ],\n",
      "  \"mitigations\": [\n",
      "    \"استخدام آليات نقل آمنة (مثل التشفير والتحقق من السلامة) لنقل النموذج والبيانات.\",\n",
      "    \"تنفيذ استراتيجية لإدارة التغيير وتحديث الأمان، مع عمليات تدقيق يدوية.\",\n",
      "    \"توفير التدريب والتوثيق للموظفين المسؤولين عن إدارة النموذج.\",\n",
      "    \"إجراء اختبارات أداء شاملة وتخطيط للطاقة لضمان قدرة البنية التحتية المحلية على تلبية المتطلبات.\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "MODEL: gemma3:12b\n",
      "PROMPT:\n",
      " Explain in English the risks of deploying local LLMs in air-gapped networks, then summarize the explanation in Arabic in 5 bullet points.\n",
      "\n",
      "RESPONSE:\n",
      " Okay, let's break down the risks of deploying Local Large Language Models (LLMs) in air-gapped networks, followed by a summary in Arabic.\n",
      "\n",
      "**Understanding the Context: Air-Gapped Networks**\n",
      "\n",
      "An air-gapped network is a computer network that is physically isolated from the internet and other external networks. The idea is to provide a high level of security by preventing remote access and data exfiltration. They are common in environments where security is paramount, like military systems, critical infrastructure, and research labs dealing with sensitive data.\n",
      "\n",
      "**Risks of Deploying LLMs in Air-Gapped Networks**\n",
      "\n",
      "While deploying an LLM in an air-gapped environment *seems* inherently secure, it introduces a unique set of risks. It's not as straightforward as \"no internet = no problem.\"  Here’s a detailed explanation of those risks, categorized for clarity:\n",
      "\n",
      "**1. Data Poisoning & Model Corruption (The Biggest Risk):**\n",
      "\n",
      "*   **The Problem:** LLMs are trained on massive datasets.  If an attacker can introduce malicious data into the LLM’s training data *within* the air-gapped network, they can subtly alter the model's behavior. This is \"data poisoning.\" It doesn't necessarily require the attacker to understand the LLM's architecture or training process. They just need to get bad data in.\n",
      "*   **How it Happens:** This can occur through:\n",
      "    *   **Compromised Data Sources:**  The LLM is initially built with pre-existing data. If that initial data source was compromised before deployment, the vulnerability is already embedded.\n",
      "    *   **Insider Threats:** A malicious insider (or a compromised insider) could introduce altered data through authorized access points (e.g., data entry clerks, system administrators).\n",
      "    *   **Supply Chain Attacks:**  Malware can be introduced within files transferred into the air-gapped network, potentially targeting data used to further refine the LLM or as prompts to test the LLM.\n",
      "*   **Consequences:** The model could start generating biased, misleading, or even harmful outputs. It could be manipulated to leak information or execute commands disguised as natural language responses.  This is especially dangerous if the LLM is used for decision-making.\n",
      "\n",
      "**2. \"Sneak-Out\" via Subtle Exfiltration Techniques:**\n",
      "\n",
      "*   **The Problem:** While direct network connections are blocked, attackers can still try to \"sneak out\" data using unusual methods. This is more likely with increasingly sophisticated attackers.\n",
      "*   **How it Happens:**\n",
      "    *   **Stenography:**  Hiding data within image files, audio files, or even seemingly innocuous text generated by the LLM.  This data can then be physically extracted (e.g., a compromised employee prints out a document containing hidden data and takes it out of the network).  LLMs *generate* images and text - perfect carriers for hidden information.\n",
      "    *   **Unintended Data Leakage:** The LLM itself could be manipulated into generating outputs that reveal sensitive information, inadvertently. A carefully crafted prompt could coax the model into regurgitating data.\n",
      "    *   **Malware Exploiting Hardware:**  Malware embedded on other devices within the air-gapped network could use hardware vulnerabilities (e.g., USB interfaces, printers) to exfiltrate data, potentially leveraging the LLM's output as a carrier.\n",
      "*   **Consequences:**  Sensitive data (strategic plans, trade secrets, classified information) could be stolen without a direct network connection.\n",
      "\n",
      "**3.  Reliance on External Tools & Dependencies:**\n",
      "\n",
      "*   **The Problem:** Even if the LLM itself is hosted locally, many tools used to interact with, monitor, or fine-tune it might initially come from outside the air-gapped network.\n",
      "*   **How it Happens:**\n",
      "    *   **Compromised Development Tools:**  Software development kits (SDKs), libraries, or frameworks used to build or interface with the LLM could contain vulnerabilities or backdoors.\n",
      "    *   **Data Serialization/Deserialization:** Formats like JSON or YAML are commonly used for data transfer.  Vulnerable deserialization routines could be exploited.\n",
      "*   **Consequences:**  A seemingly harmless development tool could introduce a security vulnerability that allows an attacker to compromise the entire system.\n",
      "\n",
      "**4.  Hardware Vulnerabilities:**\n",
      "\n",
      "*   **The Problem:** The hardware running the LLM (servers, GPUs) might have vulnerabilities that can be exploited.\n",
      "*   **How it Happens:**\n",
      "    *   **Firmware Exploits:**  Firmware vulnerabilities in the hardware itself (e.g., GPU firmware) can be exploited to gain control of the system.\n",
      "    *   **Side-Channel Attacks:**  These attacks exploit physical characteristics of the hardware (e.g., power consumption, electromagnetic radiation) to extract information.\n",
      "*   **Consequences:**  An attacker could potentially gain control of the LLM and the entire system, even without direct network access.\n",
      "\n",
      "**5.  Human Error & Insider Threats:**\n",
      "\n",
      "*   **The Problem:**  The weakest link in any security system is often human error.\n",
      "*   **How it Happens:**\n",
      "    *   **Accidental Data Introduction:** An employee might inadvertently introduce malicious data through a USB drive or other removable media.\n",
      "    *   **Social Engineering:**  An attacker could manipulate an employee into performing actions that compromise the security of the system.\n",
      "*   **Consequences:**  A single mistake by an employee can bypass all the technical security controls in place.\n",
      "\n",
      "\n",
      "\n",
      "**Arabic Summary (5 Bullet Points)**\n",
      "\n",
      "Here's a summary of the risks in Arabic, followed by a transliteration for those who are learning Arabic:\n",
      "\n",
      "*   **تسميم البيانات وتشويه النموذج:** أكبر خطر، حيث يمكن إدخال بيانات ضارة إلى النموذج، مما يؤثر على سلوكه.\n",
      "    *(Tasmeem al-bayānati wa tashwīh an-namūdhaj: Akbar khaṭar, ḥaythu yumkin idkhāl bayānāt ḍarrāʾ ila an-namūdhaj, mimmā yuʾaththir ʿalā sulūkhi.)*  (Data poisoning and model corruption: The biggest risk, where malicious data can be injected into the model, affecting its behavior.)\n",
      "*   **تسريب البيانات بشكل خفي:** محاولات إخراج البيانات من خلال طرق غير مباشرة، مثل إخفاءها في الصور أو النصوص.\n",
      "    *(Tasrīb al-bayānāt bi-shakl khafī: Muḥāwalāt ikhrāj al-bayānāt min khilāl ṭuruq ghayr mubāshirah, mithl ikhfāʾihā fī aṣ-ṣuwār aw an-nuṣūṣ.)* (Sneak-out of data covertly: Attempts to extract data through indirect methods, such as hiding it in images or text.)\n",
      "*   **الاعتماد على أدوات خارجية:** الأدوات المستخدمة في التفاعل مع النموذج قد تكون ملوثة أو تحتوي على ثغرات.\n",
      "    *(Al-iʿtimād ʿalā adawāt khārijiyyah: Al-adawāt al-mustakhdamah fī at-tafāʿul maʿa an-namūdhaj qad takūn malūthah aw taḥtawī ʿalā thughar.)* (Reliance on external tools: The tools used to interact with the model may be contaminated or contain vulnerabilities.)\n",
      "*   **نقاط ضعف في الأجهزة:** وجود ثغرات في الأجهزة التي تشغل النموذج، مثل الأجهزة الصلبة أو البرامج الثابتة.\n",
      "    *(Naqāṭ ḍaʿf fī al-ajhizah: Wujūd thughar fī al-ajhizah allatī tushagghil an-namūdhaj, mithl al-ajhizah aṣ-ṣulbah aw al-barāmij ath-thābitah.)* (Weaknesses in hardware: Existence of vulnerabilities in the hardware running the model, such as solid devices or firmware.)\n",
      "*   **الأخطاء البشرية والتهديدات الداخلية:**  الخطأ البشري أو التلاعب من قبل موظفين داخليين يمثل تهديداً كبيراً.\n",
      "    *(Al-akhṭāʾ al-bashariyyah wa at-tahdīdāt ad-dākhiliyyah: Al-akhṭāʾ al-basharī aw at-talʿīb min qibal muwaẓẓafīn dākhiliyyīn yumaththil tahdīdan kabīran.)* (Human errors and insider threats: Human error or manipulation by internal employees poses a significant threat.)\n",
      "\n",
      "\n",
      "\n",
      "**Important Note:**  Deploying LLMs in air-gapped environments is highly complex and requires a layered defense-in-depth strategy. It's not sufficient to simply isolate the network; you need to rigorously control all data flows, implement strict access controls, regularly audit the system, and train personnel on security awareness.\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "MODEL: gemma3:12b\n",
      "PROMPT:\n",
      " Create a decision tree in text format for choosing between:\n",
      "1) Small model (under 5GB)\n",
      "2) Medium model (8-12B)\n",
      "3) Large model (27B+)\n",
      "\n",
      "Consider latency, GPU memory, number of users, and reasoning quality.\n",
      "\n",
      "RESPONSE:\n",
      " Okay, here's a decision tree in text format for choosing between Small, Medium, and Large language models, considering latency, GPU memory, number of users, and reasoning quality. I've tried to structure it to be reasonably comprehensive and easy to follow.  **Please read the notes at the bottom after the tree – they are crucial for understanding the nuances and potential adjustments needed.**\n",
      "\n",
      "```\n",
      "**START: Choose a Language Model**\n",
      "\n",
      "1. **GPU Memory Available (Rough Estimate)**\n",
      "   *  < 4GB:  -> **SMALL MODEL (Under 5GB)**  (Limited options, prioritizes feasibility)\n",
      "   *  4-16GB:  -> **Question 2**\n",
      "   *  > 16GB: -> **Question 3**\n",
      "\n",
      "2. **Number of Concurrent Users (Expected Peak)**\n",
      "    * < 5 Users: -> **Question 4**\n",
      "    * 5-20 Users: -> **MEDIUM MODEL (8-12B)** (Good balance)\n",
      "    * > 20 Users: -> **Question 5**\n",
      "\n",
      "3. **Reasoning Quality is ABSOLUTELY Critical (e.g., code generation, complex planning)**\n",
      "    * YES: -> **LARGE MODEL (27B+)** (Prioritizes quality, potentially at cost)\n",
      "    * NO:  -> **Question 6**\n",
      "\n",
      "4. **Latency Requirements**\n",
      "    *  < 200ms (Near Real-Time): -> **SMALL MODEL (Under 5GB)** (Latency is primary concern)\n",
      "    *  200ms - 1 Second: -> **MEDIUM MODEL (8-12B)** (Acceptable latency, reasonable quality)\n",
      "    *  > 1 Second: -> **SMALL MODEL (Under 5GB)** (May be able to optimize small model)\n",
      "\n",
      "5. **Latency Requirements** (After exceeding 20 users)\n",
      "    *  < 200ms (Near Real-Time): -> **MEDIUM MODEL (8-12B)** (Needs optimization, explore quantization)\n",
      "    *  200ms - 1 Second: -> **LARGE MODEL (27B+)** (Consider distributed inference)\n",
      "    *  > 1 Second: -> **Evaluate tradeoffs, consider splitting load with a smaller model for some users.**\n",
      "\n",
      "6. **Prioritization: Latency vs. Reasoning Quality (If Not Critical)**\n",
      "   * **Latency is HIGHER Priority:** -> **MEDIUM MODEL (8-12B)** (balance)\n",
      "   * **Reasoning Quality is Higher Priority:** -> **LARGE MODEL (27B+)** (sacrifice some latency)\n",
      "\n",
      "**END: Model Choice**\n",
      "```\n",
      "\n",
      "**IMPORTANT NOTES & Considerations - *READ THESE CAREFULLY***\n",
      "\n",
      "* **GPU Memory is a Constraint, Not a Guarantee:**  The memory estimates are *very* rough.  Model quantization (e.g., INT8, FP16) *can* significantly reduce memory footprint, but also can impact quality.  Actual memory usage depends on the specific model architecture, libraries used, batch size, and other factors.  Always test!\n",
      "* **Number of Users is a Critical Metric:** This is a *peak* expectation.  Sudden spikes can overload a system.  Horizontal scaling (multiple instances) is often needed for higher user loads.\n",
      "* **Reasoning Quality is Subjective:** \"Reasoning quality\" is hard to define precisely.  Consider tasks like complex code generation, creative writing with nuanced reasoning, or advanced problem-solving. If you're doing simple chat/generation, reasoning quality may be less critical.\n",
      "* **Latency is a User Experience Factor:**  Lower latency = better responsiveness.  But exceeding acceptable latency is a bigger issue than very slightly faster responses.  Perception of latency matters, too – short delays *feel* longer than they are.\n",
      "* **Tradeoffs are ALWAYS Necessary:**  There's no perfect choice.  You'll be trading off speed, memory usage, quality, and cost.  This decision tree helps identify the direction to prioritize.\n",
      "* **Model Specifics:**  Model architectures (e.g., Llama, Mistral, Gemini) have different memory profiles and performance characteristics. This tree assumes a general baseline.\n",
      "* **Quantization:**  Using lower precision (e.g., INT8 instead of FP16) reduces memory but can decrease quality. Experiment to find the right balance.\n",
      "* **Distributed Inference:** Large models might need to be spread across multiple GPUs. This adds complexity.\n",
      "* **Dynamic Scaling:** Implement a system that can automatically adjust the model deployed based on load.\n",
      "* **Testing & Benchmarking:**  *Critically important.*  These are guidelines, not gospel. Test the models under realistic workloads to validate performance.  Don't just rely on theoretical numbers.\n",
      "* **Cost:** Larger models consume more GPU resources, which translates to higher costs. Factor in the long-term operational expenses.\n",
      "* **Fine-tuning:**  Fine-tuning a smaller model on a specific task can sometimes achieve reasoning quality comparable to a larger, general-purpose model.\n",
      "\n",
      "\n",
      "\n",
      "This decision tree provides a starting point. Adjust it based on your specific needs and constraints. Good luck!\n",
      "============================================================\n",
      "\n",
      "\n",
      "Embedding vector size: 1024\n",
      "\n",
      "Results saved to Excel.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import time\n",
    "import psutil\n",
    "import subprocess\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import openpyxl\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "REPORT_PATH = r\"Models Report.xlsx\"\n",
    "TESTER_NAME = \"Rami\"\n",
    "DEVICE = \"Windows PC (RTX 5070 Ti, Ryzen 9 9950X, 32GB RAM)\"\n",
    "\n",
    "CHAT_MODEL = \"gemma3:12b\"\n",
    "#! EMBED_MODEL = \"nomic-embed-text\"\n",
    "EMBED_MODEL = \"bge-m3\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# TEST PROMPTS (English + Arabic)\n",
    "# =========================\n",
    "\n",
    "CHAT_TESTS = [\n",
    "\n",
    "    {\n",
    "        \"name\": \"Arabic SOP\",\n",
    "        \"prompt\": (\n",
    "            \"أنت مساعد ذكاء اصطناعي يعمل داخل شركة تعتمد نظام ذكاء اصطناعي محلي.\\n\"\n",
    "            \"اكتب إجراء تشغيلي قياسي (SOP) للتعامل مع حادث تسريب بيانات محتمل.\\n\"\n",
    "            \"المتطلبات:\\n\"\n",
    "            \"1) سبع خطوات مرقمة فقط.\\n\"\n",
    "            \"2) جدول قرار مبسط يحتوي على (منخفض، متوسط، عالي).\\n\"\n",
    "            \"3) لغة رسمية واحترافية بدون مبالغة.\\n\"\n",
    "        )\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"name\": \"Arabic Reasoning Deep\",\n",
    "        \"prompt\": (\n",
    "            \"لدينا خياران لإدارة المعرفة الداخلية:\\n\"\n",
    "            \"أ) استخدام RAG مع قاعدة بيانات متجهات.\\n\"\n",
    "            \"ب) استخدام Fine-Tuning عبر LoRA.\\n\\n\"\n",
    "            \"قارن بين الخيارين من حيث:\\n\"\n",
    "            \"الأمان، الصيانة، الدقة، التكلفة، وسهولة الترقية.\\n\"\n",
    "            \"ثم قدم توصية واضحة مع تبرير منطقي من 5 نقاط.\"\n",
    "        )\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"name\": \"Arabic JSON Strict\",\n",
    "        \"prompt\": (\n",
    "            \"أعد النتيجة بصيغة JSON فقط بدون أي نص إضافي.\\n\"\n",
    "            \"المفاتيح المطلوبة:\\n\"\n",
    "            \"summary (string), risks (array of 4 strings), mitigations (array of 4 strings).\\n\"\n",
    "            \"الموضوع: نشر نموذج ذكاء اصطناعي محلي داخل بيئة Air-Gapped.\"\n",
    "        )\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"name\": \"Mixed Language Stress\",\n",
    "        \"prompt\": (\n",
    "            \"Explain in English the risks of deploying local LLMs in air-gapped networks, \"\n",
    "            \"then summarize the explanation in Arabic in 5 bullet points.\"\n",
    "        )\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"name\": \"Logical Structure Test\",\n",
    "        \"prompt\": (\n",
    "            \"Create a decision tree in text format for choosing between:\\n\"\n",
    "            \"1) Small model (under 5GB)\\n\"\n",
    "            \"2) Medium model (8-12B)\\n\"\n",
    "            \"3) Large model (27B+)\\n\\n\"\n",
    "            \"Consider latency, GPU memory, number of users, and reasoning quality.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "EMBED_TEST = {\n",
    "    \"name\": \"Arabic Embedding\",\n",
    "    \"prompt\": \"سياسة الإجازات السنوية وآلية الموافقة عليها داخل الشركة.\"\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# METRIC SAMPLER\n",
    "# =========================\n",
    "\n",
    "def query_vram():\n",
    "    try:\n",
    "        out = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.used\", \"--format=csv,noheader,nounits\"]\n",
    "        )\n",
    "        vals = [int(x.strip()) for x in out.decode().splitlines()]\n",
    "        return max(vals)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "class PeakSampler:\n",
    "    def __init__(self):\n",
    "        self.stop_flag = False\n",
    "        self.peak_vram = 0\n",
    "        self.peak_ram = 0\n",
    "        self.peak_cpu = 0\n",
    "\n",
    "    def start(self):\n",
    "        threading.Thread(target=self.sample, daemon=True).start()\n",
    "\n",
    "    def sample(self):\n",
    "        process = psutil.Process()\n",
    "        while not self.stop_flag:\n",
    "            vram = query_vram()\n",
    "            if vram:\n",
    "                self.peak_vram = max(self.peak_vram, vram)\n",
    "            ram = psutil.virtual_memory().used / (1024**3)\n",
    "            self.peak_ram = max(self.peak_ram, ram)\n",
    "            cpu = process.cpu_percent(interval=None)\n",
    "            self.peak_cpu = max(self.peak_cpu, cpu)\n",
    "            time.sleep(0.2)\n",
    "\n",
    "    def stop(self):\n",
    "        self.stop_flag = True\n",
    "\n",
    "\n",
    "# =========================\n",
    "# TEST FUNCTIONS\n",
    "# =========================\n",
    "\n",
    "def test_chat(model, prompt):\n",
    "    sampler = PeakSampler()\n",
    "    sampler.start()\n",
    "\n",
    "    start = time.time()\n",
    "    error = \"\"\n",
    "    full_output = \"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        full_output = response[\"message\"][\"content\"]\n",
    "\n",
    "        # Print full output to console\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"MODEL: {model}\")\n",
    "        print(\"PROMPT:\\n\", prompt)\n",
    "        print(\"\\nRESPONSE:\\n\", full_output)\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "\n",
    "    sampler.stop()\n",
    "    elapsed = round(time.time() - start, 2)\n",
    "\n",
    "    return {\n",
    "        \"Errors/Warnings\": error or \"None\",\n",
    "        \"Output Sample\": full_output[:300],\n",
    "        \"Response Time\": f\"{elapsed} sec\",\n",
    "        \"VRAM Usage\": f\"{sampler.peak_vram} MB\",\n",
    "        \"RAM Usage\": f\"{round(sampler.peak_ram,2)} GB\",\n",
    "        \"CPU Usage\": f\"{round(sampler.peak_cpu,1)} %\",\n",
    "    }\n",
    "\n",
    "def test_embed(model, prompt):\n",
    "    start = time.time()\n",
    "    error = \"\"\n",
    "    try:\n",
    "        response = ollama.embeddings(model=model, prompt=prompt)\n",
    "        size = len(response[\"embedding\"])\n",
    "        print(f\"\\nEmbedding vector size: {size}\\n\")\n",
    "        output = f\"Embedding size: {size}\"\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        output = \"\"\n",
    "    elapsed = round(time.time() - start, 2)\n",
    "    return {\n",
    "        \"Errors/Warnings\": error or \"None\",\n",
    "        \"Output Sample\": output,\n",
    "        \"Response Time\": f\"{elapsed} sec\",\n",
    "        \"VRAM Usage\": \"N/A\",\n",
    "        \"RAM Usage\": \"N/A\",\n",
    "        \"CPU Usage\": \"N/A\",\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# SAVE TO EXCEL\n",
    "# =========================\n",
    "\n",
    "def append_to_excel(rows):\n",
    "    wb = openpyxl.load_workbook(REPORT_PATH)\n",
    "    ws = wb.active\n",
    "\n",
    "    for row in rows:\n",
    "        ws.append([\n",
    "            datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            TESTER_NAME,\n",
    "            row[\"Errors/Warnings\"],\n",
    "            row[\"Output Sample\"],\n",
    "            row[\"Response Time\"],\n",
    "            row[\"VRAM Usage\"],\n",
    "            row[\"RAM Usage\"],\n",
    "            row[\"CPU Usage\"],\n",
    "            DEVICE,\n",
    "            row.get(\"Prompt Used\",\"\"),\n",
    "            row.get(\"Model Name\",\"\")\n",
    "        ])\n",
    "\n",
    "    wb.save(REPORT_PATH)\n",
    "\n",
    "# =========================\n",
    "# RUN\n",
    "# =========================\n",
    "\n",
    "def main():\n",
    "    rows = []\n",
    "\n",
    "    for t in CHAT_TESTS:\n",
    "        result = test_chat(CHAT_MODEL, t[\"prompt\"])\n",
    "        result[\"Prompt Used\"] = t[\"prompt\"]\n",
    "        result[\"Model Name\"] = CHAT_MODEL\n",
    "        rows.append(result)\n",
    "\n",
    "    embed_result = test_embed(EMBED_MODEL, EMBED_TEST[\"prompt\"])\n",
    "    embed_result[\"Prompt Used\"] = EMBED_TEST[\"prompt\"]\n",
    "    embed_result[\"Model Name\"] = EMBED_MODEL\n",
    "    rows.append(embed_result)\n",
    "\n",
    "    append_to_excel(rows)\n",
    "    print(\"Results saved to Excel.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0c7deb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
